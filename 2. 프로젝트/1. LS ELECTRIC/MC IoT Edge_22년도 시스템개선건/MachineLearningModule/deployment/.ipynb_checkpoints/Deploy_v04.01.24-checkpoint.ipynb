{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "allied-jones",
   "metadata": {},
   "source": [
    "## 추가해야하는 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "traditional-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.24.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "import os\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-substance",
   "metadata": {},
   "source": [
    "# 1. Azure ML Service 작업 영업에 연결 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-chess",
   "metadata": {},
   "source": [
    "#### Notebook이 실행되는 경로 내 Azure ML Service Workspace 접속 정보를 가진 config 파일이 있어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-wildlife",
   "metadata": {},
   "source": [
    " - 테스트 배포 시에는 'data/config_dev.json' 사용\n",
    " - 실제 배포 시에는 'data/config.json' 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "received-equivalent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기계학습이름:mltest21\n",
      "리소스그룹이름:bigdatateam\n",
      "리전이름:koreacentral\n",
      "구독ID:245d9749-ca17-42f2-ae5e-2aefd9de113b\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config('config/config_dev_test.json')\n",
    "print('기계학습이름:'+ws.name, '리소스그룹이름:'+ws.resource_group, '리전이름:'+ws.location, '구독ID:'+ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instant-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"c8b9cf2c-f80b-4c36-912f-74979c640070\")\n",
    "\n",
    "ws = Workspace(subscription_id=\"1f154107-bef0-481f-a741-68e7ba34affe\",\n",
    "               resource_group=\"lsmc-prd-rg\",\n",
    "               workspace_name=\"lsmc-dev-mlws\",\n",
    "               auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tight-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"54af7187-79bd-47bd-aea7-8fb194e64cdd\")\n",
    "\n",
    "ws = Workspace(subscription_id=\"245d9749-ca17-42f2-ae5e-2aefd9de113b\",\n",
    "               resource_group=\"bigdatateam\",\n",
    "               workspace_name=\"mltest21\",\n",
    "               auth=interactive_auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-suspect",
   "metadata": {},
   "source": [
    "# 2. 모델 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-category",
   "metadata": {},
   "source": [
    "#### 등록할 모델의 버전을 명시해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broadband-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_051_01.pickle\n"
     ]
    }
   ],
   "source": [
    "model_version = \"03_051_01\"\n",
    "file_name = '{}.pickle'.format(model_version)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-spray",
   "metadata": {},
   "source": [
    "## Description 수정해아함!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-happiness",
   "metadata": {},
   "source": [
    "#### 모델을 등록해준다. description에 현재 모델에 대한 간단한 설명을 추가할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-scratch",
   "metadata": {},
   "source": [
    " - 실행하면, [Azure Portal > 기계 학습 > 기계학습이름(위에서 확인) > 모델] 에서 확인 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exempt-pregnancy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model 03_051_01\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "#library_version = \"DL\"+sklearn.__version__.replace(\".\",\"x\")\n",
    "model = Model.register(model_path = \"data/pkl/{}\".format(file_name),\n",
    "                       model_name = model_version,\n",
    "                       tags = {'area': \"IoT Edge\", 'type': \"azureml-automl\"},\n",
    "                       description = \"ML=03_051_01, Sys=04.01.00\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-hampshire",
   "metadata": {},
   "source": [
    "# 3. 이미지 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-packaging",
   "metadata": {},
   "source": [
    "#### 등록 된 모델 이름을 사용 하 여 모델 파일에 대 한 경로를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prostate-focus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml-models\\\\03_051_01\\\\28\\\\03_051_01.pickle'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model.get_model_path(model_version, _workspace=ws)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-infrastructure",
   "metadata": {},
   "source": [
    "#### 사용되는 package 선언. Edge 환경에서 해당 package를 사용되기 때문에 channel 과 필요한 package명을 선언 해주어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "broadband-screw",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip packages: ['azureml-defaults', 'azureml==0.2.7', 'azureml-core==1.24.0', 'azureml-sdk==1.24.0', 'azureml-automl-core==1.24.0', 'azureml-automl-runtime==1.24.0', 'azure-storage-blob==12.6.0', 'numpy==1.18.5', 'pandas==0.25.3', 'sqlalchemy==1.3.21', 'h5py==2.10.0', 'tqdm==4.54.0', 'obspy==1.2.1', 'setuptools==45.3.0', 'mysql-connector-python==8.0.18', 'joblib==0.14.1', 'packaging==20.7', 'xgboost==0.90', 'noisereduce==1.1.0']\n",
      "conda packasges: ['gxx_linux-64', 'gcc_linux-64', 'librosa==0.8.0', 'pyyaml']\n"
     ]
    }
   ],
   "source": [
    "pip_packages = [\"azureml-defaults\",\n",
    "                \"azureml==0.2.7\",\n",
    "                \"azureml-core==1.24.0\",\n",
    "                \"azureml-sdk==1.24.0\",\n",
    "                \"azureml-automl-core==1.24.0\",\n",
    "                \"azureml-automl-runtime==1.24.0\",\n",
    "                \"azure-storage-blob==12.6.0\",\n",
    "                \"numpy==1.18.5\",\n",
    "                \"pandas==0.25.3\",\n",
    "                \"sqlalchemy==1.3.21\",\n",
    "                \"h5py==2.10.0\",\n",
    "                \"tqdm==4.54.0\",\n",
    "                \"obspy==1.2.1\",\n",
    "                \"setuptools==45.3.0\",\n",
    "                \"mysql-connector-python==8.0.18\",\n",
    "                \"joblib==0.14.1\",\n",
    "                \"packaging==20.7\",\n",
    "                \"xgboost==0.90\",\n",
    "                \"noisereduce==1.1.0\"]\n",
    "\n",
    "lib_config_load = ['pyyaml']\n",
    "lib_clfs = [\"gxx_linux-64\",\n",
    "            \"gcc_linux-64\",\n",
    "            \"librosa==0.8.0\"]\n",
    "#lib_clfs = ['tensorflow==1.1']\n",
    "conda_packages = lib_clfs + lib_config_load\n",
    "\n",
    "print('pip packages:', pip_packages)\n",
    "print('conda packasges:', conda_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "laughing-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core import Environment\n",
    "\n",
    "conda_deps  = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "conda_deps.add_channel('conda-forge')\n",
    "conda_deps.add_channel('defaults')\n",
    "\n",
    "myenv = Environment(name='myenv')\n",
    "myenv.python.conda_dependencies = conda_deps\n",
    "\n",
    "# myenv.docker.base_image = 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04'\n",
    "\n",
    "\n",
    "#myenv.docker.base_image = 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04'\n",
    "# myenv.docker.base_image = 'mcr.microsoft.com/azureml/base:openmpi3.1.2-ubuntu18.04'\n",
    "\n",
    "\n",
    "# with open(\"data/myenv.yml\",\"w\") as f:\n",
    "#     f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-bobby",
   "metadata": {},
   "source": [
    "# 4. score_iot.py 파일(전처리 및 ML 판정 수행 로직 실행 스크립트) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-divorce",
   "metadata": {},
   "source": [
    " ### 아래 cell 이 실행되면 score_iot.py에 실행 스크립트가 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-characteristic",
   "metadata": {},
   "source": [
    "### score_iot_test.ipynb에 해당 스크립트의 내용을 복사하여, 실행 시 에러가 발생하지 않는지 꼭 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confirmed-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _locale\n",
    "_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acknowledged-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/score_iot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/score_iot.py\n",
    "\n",
    "# For Edge\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# For Clf\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from classify import Classifier\n",
    "from preprocess_signal_data import Run_\n",
    "from pkgs.pipelines import FeatureDeployed as Featurizing\n",
    "from pkgs.utils import read_json\n",
    "\n",
    "def read_data(input_path, attempt):\n",
    "    print(\"Attempt: \" + str(attempt + 1) + \"  Time: \" + str(datetime.now()))\n",
    "    with h5py.File(input_path, \"r\") as f:\n",
    "        tmp = f[\"Raw\"][:]\n",
    "    input_raw = pd.DataFrame(tmp, columns=[0, 1, 2])\n",
    "    print(\"\\n\", \"hdf loaded\")\n",
    "    \n",
    "    return input_raw\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def init_preprocessor(feature_params, Featurizing, test_no, input_type=\"ReadData\"):\n",
    "    # set preprocess class\n",
    "    preproc = Run_(\n",
    "        feature_params=feature_params, Featurizing=Featurizing, test_no=test_no\n",
    "    )\n",
    "    if input_type == \"ReadData\":\n",
    "        preproc.pl.steps = [\n",
    "            (step_name, step)\n",
    "            for step_name, step in preproc.pl.steps\n",
    "            if step.name != \"ReadData\"\n",
    "        ]\n",
    "\n",
    "    return preproc\n",
    "\n",
    "\n",
    "def score(\n",
    "    preproc,\n",
    "    input_values,\n",
    "    feature_names,\n",
    "    column_names,\n",
    "    cut_off,\n",
    "    rule_model_params,\n",
    "    mode,\n",
    "    input_name=\"ReadData\",\n",
    "    parallel=1,\n",
    "    verbose=0,\n",
    "):\n",
    "    # Preprocess\n",
    "    df = preproc.pl.run(\n",
    "        input_name=input_name,\n",
    "        input_values=[input_values],\n",
    "        feature_names=feature_names,\n",
    "        parallel=parallel,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    df = pd.DataFrame(df[0])\n",
    "    df[\"TEST_NO\"] = [0,1,2]\n",
    "    df_features = df[column_names]\n",
    "\n",
    "    # Predict\n",
    "    df[\"PROB\"] = loaded_model.predict_proba(df_features)[:,1]\n",
    "    \n",
    "    # Classify\n",
    "    df[\"ML_R\"] = df.PROB >= cut_off\n",
    "    df[\"ML_R\"] = df.ML_R.astype(int)\n",
    "    \n",
    "    df_classified = Classifier.classify_all(\n",
    "        df=df[df.TEST_NO.isin(preproc.featurizer.params[\"test_no\"])],\n",
    "        rule_cutoffs=rule_model_params,\n",
    "        mode=mode,\n",
    "    )\n",
    "    return df_classified, df\n",
    "\n",
    "\n",
    "def init():\n",
    "    global preproc, loaded_model, model_version, column_names, line, cut_off, rule_model_params, ml_model_params, model_path, error_dir\n",
    "    # 수정필요  --------------------------------------------------------\n",
    "    model_version = \"03_051_01\"  # model_name 입력\n",
    "    file_name = \"{}.pickle\".format(model_version)\n",
    "    edge_config = \"/home/data/edge_config.yml\"\n",
    "    feature_params = read_json(\"src/config/feature051_parameters.json\")\n",
    "    column_names = read_json(\"src/config/model_03_051_01_features.json\")[\"feature_names\"]\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # load line info\n",
    "    with open(edge_config, \"r\") as stream:\n",
    "        try:\n",
    "            edge_config = yaml.load(stream, Loader=yaml.BaseLoader)\n",
    "            line = edge_config[\"config\"][\"line\"][\"name\"]\n",
    "            cut_off = float(edge_config[\"config\"][\"param\"][\"cutoff\"])\n",
    "            rule_model_params = edge_config[\"config\"][\"param\"][\"rule_model\"]\n",
    "            ml_model_params = edge_config[\"config\"][\"param\"][\"ml_model\"]\n",
    "\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(\"line config error: \", exc)\n",
    "\n",
    "    # set preprocess class\n",
    "    for param_, dic_ in ml_model_params.items():\n",
    "        for key_, value_ in dic_.items():\n",
    "            feature_params[param_][key_] = float(value_)\n",
    "    preproc = init_preprocessor(\n",
    "        feature_params=feature_params, Featurizing=Featurizing, test_no=[0, 1, 2]\n",
    "    )\n",
    "\n",
    "    # load model\n",
    "    model_path = Model.get_model_path(file_name)\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    \n",
    "    error_dir = \"/home/data/error_file/\"\n",
    "    try:\n",
    "        os.makedirs(error_dir, mode=777)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def run(input_json):\n",
    "    print(\"\\n\", \"mlmodule start\")\n",
    "    print(\"\\n\", datetime.now() + timedelta(hours=9), \"\\n\")\n",
    "    # for test#\n",
    "    input_json = json.loads(input_json)\n",
    "    print(\"\\n\", \"json loaded\", \"\\n\")\n",
    "    print(input_json, \"\\n\")\n",
    "    mltime = datetime.now() + timedelta(hours=9)\n",
    "    chtime = input_json[\"chtime\"]\n",
    "    print(\"chtime : \", chtime)\n",
    "    ct = datetime.strptime(\n",
    "        chtime.replace(\"T\", \" \").split(\"+\")[0][:-1], \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    )\n",
    "    diff = mltime - ct\n",
    "    input_json[\"chtime\"] = str(ct)\n",
    "    input_json[\"mltime\"] = str(mltime)\n",
    "    input_json[\"etime_ch\"] = diff.seconds + diff.microseconds / 1e6\n",
    "\n",
    "    # file load\n",
    "    init_time = time.time()\n",
    "    # input_json = json.loads(input_json)\n",
    "    input_path = input_json[\"path\"]\n",
    "    print(\"\\n\", input_path)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            input_raw = read_data(input_path, attempt)\n",
    "\n",
    "        except Exception as ex:\n",
    "            if attempt == 2:\n",
    "                ex_message = str(ex)\n",
    "\n",
    "            sleep(0.02)\n",
    "            continue\n",
    "\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        input_json[\"b\"] = 2\n",
    "        input_json[\"prob\"] = 2\n",
    "        input_json[\"error\"] = ex_message\n",
    "        input_json[\"etime\"] = time.time() - init_time\n",
    "\n",
    "        result_json = [json.dumps(input_json)]\n",
    "        print(\"*\" * 5, \" \", \"LOAD ERROR\", \" \", \"*\" * 5)\n",
    "        print(result_json)\n",
    "        try:\n",
    "            shutil.copy(input_path, error_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # for test#\n",
    "    # diff = (datetime.now()+ timedelta(hours=9))-mltime\n",
    "    # input_json['etime_load'] = diff.seconds + diff.microseconds/1E6\n",
    "    input_json[\"etime_load\"] = time.time() - init_time\n",
    "\n",
    "    ### json insert\n",
    "    input_json[\"cutoff\"] = cut_off\n",
    "    input_json[\"FTUR_ENRG_TRHD\"] = float(rule_model_params[\"FTUR_ENRG\"])\n",
    "    input_json[\"FTUR_WVFM_STDDEV_TRHD\"] = float(rule_model_params[\"FTUR_WVFM_STDDEV\"])\n",
    "    input_json[\"FTUR_TRGER_TRHD\"] = float(rule_model_params[\"FTUR_TRGER\"])\n",
    "    \n",
    "    input_json[\"TRHD_NM_SET\"] = \"/\".join([key for key in rule_model_params.keys()] + [f\"{key_}-{param}\" for key_, dict_ in ml_model_params.items() for param, value in dict_.items()])\n",
    "    input_json[\"TRHD_VAL_SET\"] = \"/\".join(\n",
    "        [str(value) for value in rule_model_params.values()]+[str(value) for key_, dict_ in ml_model_params.items() for param, value in dict_.items()]\n",
    "    )\n",
    "    input_json[\"FTUR_NM_SET\"] = \"/\".join(\n",
    "        [\n",
    "            \"/\".join([col + \"_TEST_N1\" for col in preproc.featurizer.params[\"feature_names\"]]),\n",
    "            \"/\".join([col + \"_TEST_N2\" for col in preproc.featurizer.params[\"feature_names\"]]),\n",
    "            \"/\".join([col + \"_TEST_N3\" for col in preproc.featurizer.params[\"feature_names\"]]),\n",
    "        ]\n",
    "    )\n",
    "    input_json[\"lid\"] = line\n",
    "    \n",
    "    ##시스템 변수 반드시 수정 필요\n",
    "    input_json[\"v\"] = \"04.01.00\"\n",
    "    ##############################\n",
    "\n",
    "    try:\n",
    "        filename = input_path.split(\"/\")[-1]\n",
    "        filename = filename.strip(\"Data\\\\\").split(\"_\")\n",
    "        input_json[\"bc\"] = \"_\".join(filename[0:2])\n",
    "        input_json[\"dtfull\"] = filename[2][:-3]\n",
    "\n",
    "    except Exception as ex:  # 에러 종류\n",
    "        input_json[\"bc\"] = \"ERROR\"\n",
    "        input_json[\"dtfull\"] = str(datetime.now())\n",
    "        input_json[\"r\"] = 2\n",
    "        input_json[\"prob\"] = 2\n",
    "        input_json[\"error\"] = str(ex)\n",
    "        input_json[\"etime\"] = time.time() - init_time\n",
    "\n",
    "        result_json = [json.dumps(input_json)]\n",
    "        print(\"*\" * 5, \" \", \"FILENAME ERROR\", \" \", \"*\" * 5)\n",
    "        print(result_json)\n",
    "        try:\n",
    "            cmd = \"cp \" + input_json[\"path\"] + \" \" + error_dir\n",
    "            os.system(cmd)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        df, df_all = score(\n",
    "            preproc=preproc,\n",
    "            input_values=input_raw,\n",
    "            feature_names=preproc.featurizer.params[\"feature_names\"],\n",
    "            column_names=column_names,\n",
    "            cut_off=cut_off,\n",
    "            rule_model_params=rule_model_params,\n",
    "            mode=\"or\",\n",
    "            input_name=\"ReadData\",\n",
    "            parallel=1,\n",
    "            verbose=0,\n",
    "        )\n",
    "        df_max = df_all.max()\n",
    "        \n",
    "        input_json[\"r\"] = int(df.R.values[0])\n",
    "        input_json[\"prob\"] = float(df.PROB.values[0])\n",
    "        input_json[\"error\"] = str(df.TEST_NG_R.values[0])\n",
    "        input_json[\"etime\"] = time.time() - init_time\n",
    "        input_json[\"FTUR_VAL_SET\"] = \"/\".join(\n",
    "            [str(value) for value in df_all[preproc.featurizer.params[\"feature_names\"]].values.flatten()]\n",
    "        )\n",
    "        input_json[\"ML_R\"] = int(df.ML_R.values[0])\n",
    "        input_json[\"RULE_R\"] = int(df.RULE_R.values[0])\n",
    "        input_json[\"TEST_NG_R\"] = int(df.TEST_NG_R.values[0])\n",
    "        input_json[\"FTUR_ENRG\"] = float(df_max.FTUR_ENRG)\n",
    "        input_json[\"FTUR_WVFM_STDDEV\"] = float(df_max.FTUR_WVFM_STDDEV)\n",
    "        input_json[\"FTUR_TRGER\"] = float(df_max.FTUR_TRGER)\n",
    "\n",
    "        result_json = [json.dumps(input_json)]\n",
    "        print(result_json)\n",
    "\n",
    "    except Exception as ex:  # 에러 종류\n",
    "        input_json[\"r\"] = 2\n",
    "        input_json[\"prob\"] = 2\n",
    "        input_json[\"error\"] = str(ex)\n",
    "        input_json[\"etime\"] = time.time() - init_time\n",
    "        input_json[\"FTUR_VAL_SET\"] = \"\"\n",
    "        input_json[\"ML_R\"] = 2\n",
    "        input_json[\"RULE_R\"] = 2\n",
    "        input_json[\"TEST_NG_R\"] = 2\n",
    "        input_json[\"FTUR_ENRG\"] = -1\n",
    "        input_json[\"FTUR_WVFM_STDDEV\"] = -1\n",
    "        input_json[\"FTUR_TRGER\"] = -1\n",
    "        \n",
    "\n",
    "        result_json = [json.dumps(input_json)]\n",
    "        print(\"*\" * 5, \" \", \"ERROR\", \" \", \"*\" * 5)\n",
    "        print(result_json)\n",
    "        try:\n",
    "            cmd = \"cp \" + input_json[\"path\"] + \" \" + error_dir\n",
    "            os.system(cmd)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return result_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optional-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\limjh\\\\MC_IoTEdge\\\\MC_QIS\\\\MachineLearningModule\\\\deployment'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-typing",
   "metadata": {},
   "source": [
    "# 5. Azure MLWS에 이미지 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "diverse-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelNotFound: Model with id azureml-models\\03_051_01\\28\\03_051_01.pickle not found in provided workspace\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model 03_051_01.pickle\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score_iot.py\", environment=myenv, source_directory=\"../src\")\n",
    "package_service = Model.package(workspace=ws,\n",
    "                                models=[model],\n",
    "                                image_name=\"jh-mlmodule\",\n",
    "                                image_label='10',\n",
    "                                inference_config=inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broadband-market",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/05/17 00:53:47 Downloading source code...\n",
      "2021/05/17 00:53:48 Finished downloading source code\n",
      "2021/05/17 00:53:49 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2021/05/17 00:53:49 Successfully set up Docker network: acb_default_network\n",
      "2021/05/17 00:53:49 Setting up Docker configuration...\n",
      "2021/05/17 00:53:50 Successfully set up Docker configuration\n",
      "2021/05/17 00:53:50 Logging in to registry: 328b44081e0a40338b99edcaa606d991.azurecr.io\n",
      "2021/05/17 00:53:51 Successfully logged into 328b44081e0a40338b99edcaa606d991.azurecr.io\n",
      "2021/05/17 00:53:51 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2021/05/17 00:53:51 Scanning for dependencies...\n",
      "2021/05/17 00:53:52 Successfully scanned dependencies\n",
      "2021/05/17 00:53:52 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  67.07kB\n",
      "Step 1/18 : FROM mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210220.v1@sha256:45f047999ab2ced61a2fd0cf8d8421f796ca05b8423c1a0ac652791a321bff9a\n",
      "mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210220.v1@sha256:45f047999ab2ced61a2fd0cf8d8421f796ca05b8423c1a0ac652791a321bff9a: Pulling from azureml/intelmpi2018.3-ubuntu16.04\n",
      "4007a89234b4: Already exists\n",
      "5dfa26c6b9c9: Already exists\n",
      "0ba7bf18aa40: Already exists\n",
      "4c6ec688ebe3: Already exists\n",
      "a2874ccdee09: Pulling fs layer\n",
      "84e6fa394f53: Pulling fs layer\n",
      "cde35e537c55: Pulling fs layer\n",
      "08224915e098: Pulling fs layer\n",
      "3e72e2b08f2a: Pulling fs layer\n",
      "503a95eb7b7f: Pulling fs layer\n",
      "cac267f3f656: Pulling fs layer\n",
      "9c9189719fce: Pulling fs layer\n",
      "08224915e098: Waiting\n",
      "3e72e2b08f2a: Waiting\n",
      "503a95eb7b7f: Waiting\n",
      "cac267f3f656: Waiting\n",
      "9c9189719fce: Waiting\n",
      "cde35e537c55: Verifying Checksum\n",
      "cde35e537c55: Download complete\n",
      "a2874ccdee09: Verifying Checksum\n",
      "a2874ccdee09: Download complete\n",
      "84e6fa394f53: Verifying Checksum\n",
      "84e6fa394f53: Download complete\n",
      "503a95eb7b7f: Verifying Checksum\n",
      "503a95eb7b7f: Download complete\n",
      "cac267f3f656: Verifying Checksum\n",
      "cac267f3f656: Download complete\n",
      "08224915e098: Verifying Checksum\n",
      "08224915e098: Download complete\n",
      "9c9189719fce: Verifying Checksum\n",
      "9c9189719fce: Download complete\n",
      "a2874ccdee09: Pull complete\n",
      "84e6fa394f53: Pull complete\n",
      "3e72e2b08f2a: Verifying Checksum\n",
      "3e72e2b08f2a: Download complete\n",
      "cde35e537c55: Pull complete\n",
      "08224915e098: Pull complete\n",
      "3e72e2b08f2a: Pull complete\n",
      "503a95eb7b7f: Pull complete\n",
      "cac267f3f656: Pull complete\n",
      "9c9189719fce: Pull complete\n",
      "Digest: sha256:45f047999ab2ced61a2fd0cf8d8421f796ca05b8423c1a0ac652791a321bff9a\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210220.v1@sha256:45f047999ab2ced61a2fd0cf8d8421f796ca05b8423c1a0ac652791a321bff9a\n",
      " ---> b79635653e9e\n",
      "Step 2/18 : USER root\n",
      " ---> Running in a2bb47393358\n",
      "Removing intermediate container a2bb47393358\n",
      " ---> df5a7b8f1762\n",
      "Step 3/18 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in f80b1ff381e0\n",
      "Removing intermediate container f80b1ff381e0\n",
      " ---> 5f95a0c85c4d\n",
      "Step 4/18 : WORKDIR /\n",
      " ---> Running in cb067084fe3f\n",
      "Removing intermediate container cb067084fe3f\n",
      " ---> fc26620b179f\n",
      "Step 5/18 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 8c7b91702b12\n",
      "Step 6/18 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 04338b410268\n",
      "Removing intermediate container 04338b410268\n",
      " ---> 4cb7060c30f0\n",
      "Step 7/18 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> 95f62a017cd2\n",
      "Step 8/18 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_6b5289b2dc5454b144f8bed30cb22bbb -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in 20f9a30139c2\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n",
      "done\n",
      "Solving environment: ...working... \n",
      "done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "mkl-2019.4           | 204.1 MB  | #########6 |  96% \n",
      "mkl-2019.4           | 204.1 MB  | ########## | 100% \n",
      "gxx_impl_linux-64-7. | 19.1 MB   | ########## | 100% \n",
      "openh264-2.1.0       | 1.5 MB    | ########## | 100% \n",
      "requests-2.24.0      | 54 KB     | ########## | 100% \n",
      "cycler-0.10.0        | 13 KB     | ########## | 100% \n",
      "llvmlite-0.34.0      | 319 KB    | ########## | 100% \n",
      "libpng-1.6.37        | 364 KB    | ########## | 100% \n",
      "cffi-1.14.0          | 225 KB    | ########## | 100% \n",
      "gnutls-3.6.5         | 1.9 MB    | ########## | 100% \n",
      "nettle-3.4.1         | 6.7 MB    | ########## | 100% \n",
      "ld_impl_linux-64-2.3 | 645 KB    | ########## | 100% \n",
      "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
      "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
      "libgfortran-ng-7.3.0 | 1.3 MB    | ########## | 100% \n",
      "tbb-2020.3           | 1.4 MB    | ########## | 100% \n",
      "libflac-1.3.3        | 517 KB    | ########## | 100% \n",
      "chardet-3.0.4        | 197 KB    | ########## | 100% \n",
      "matplotlib-base-3.2. | 7.2 MB    | ########## | 100% \n",
      "pooch-1.3.0          | 40 KB     | ########## | 100% \n",
      "bzip2-1.0.8          | 105 KB    | ########## | 100% \n",
      "libgomp-9.2.0        | 816 KB    | ########## | 100% \n",
      "binutils_linux-64-2. | 24 KB     | ########## | 100% \n",
      "binutils_impl_linux- | 8.6 MB    | ########## | 100% \n",
      "ca-certificates-2020 | 128 KB    | ########## | 100% \n",
      "scipy-1.5.2          | 18.5 MB   | ########## | 100% \n",
      "gmp-6.1.2            | 744 KB    | ########## | 100% \n",
      "pysoundfile-0.10.3.p | 23 KB     | ########## | 100% \n",
      "scikit-learn-0.23.2  | 6.9 MB    | ########## | 100% \n",
      "ffmpeg-4.2.2         | 80.5 MB   | #########4 |  95% \n",
      "ffmpeg-4.2.2         | 80.5 MB   | ########## | 100% \n",
      "numpy-1.19.1         | 20 KB     | ########## | 100% \n",
      "pyyaml-5.3.1         | 191 KB    | ########## | 100% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "gettext-0.19.8.1     | 3.7 MB    | ########## | 100% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "resampy-0.2.2        | 332 KB    | ########## | 100% \n",
      "pysocks-1.7.1        | 30 KB     | ########## | 100% \n",
      "decorator-4.4.2      | 14 KB     | ########## | 100% \n",
      "gcc_linux-64-7.3.0   | 25 KB     | ########## | 100% \n",
      "lame-3.100           | 502 KB    | ########## | 100% \n",
      "packaging-20.4       | 35 KB     | ########## | 100% \n",
      "libllvm10-10.0.1     | 26.5 MB   | ########## | 100% \n",
      "certifi-2020.6.20    | 160 KB    | ########## | 100% \n",
      "audioread-2.1.9      | 32 KB     | ########## | 100% \n",
      "icu-58.2             | 22.7 MB   | ########## | 100% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "libffi-3.2.1         | 52 KB     | ########## | 100% \n",
      "yaml-0.2.5           | 87 KB     | ########## | 100% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \n",
      "x264-1!157.20191217  | 2.5 MB    | ########## | 100% \n",
      "pyparsing-2.4.7      | 64 KB     | ########## | 100% \n",
      "urllib3-1.25.11      | 93 KB     | ########## | 100% \n",
      "python_abi-3.6       | 4 KB      | ########## | 100% \n",
      "asn1crypto-1.4.0     | 77 KB     | ########## | 100% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "numba-0.51.2         | 3.7 MB    | ########## | 100% \n",
      "mkl_random-1.1.0     | 369 KB    | ########## | 100% \n",
      "freetype-2.10.4      | 901 KB    | ########## | 100% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "intel-openmp-2020.2  | 947 KB    | ########## | 100% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "kiwisolver-1.2.0     | 91 KB     | ########## | 100% \n",
      "threadpoolctl-2.1.0  | 16 KB     | ########## | 100% \n",
      "python-dateutil-2.8. | 224 KB    | ########## | 100% \n",
      "pyopenssl-19.0.0     | 82 KB     | ########## | 100% \n",
      "libogg-1.3.2         | 205 KB    | ########## | 100% \n",
      "numpy-base-1.19.1    | 5.2 MB    | ########## | 100% \n",
      "joblib-0.17.0        | 205 KB    | ########## | 100% \n",
      "brotlipy-0.7.0       | 348 KB    | ########## | 100% \n",
      "pycparser-2.20       | 94 KB     | ########## | 100% \n",
      "gxx_linux-64-7.3.0   | 25 KB     | ########## | 100% \n",
      "wheel-0.35.1         | 36 KB     | ########## | 100% \n",
      "appdirs-1.4.4        | 13 KB     | ########## | 100% \n",
      "mkl_fft-1.2.0        | 164 KB    | ########## | 100% \n",
      "librosa-0.8.0        | 145 KB    | ########## | 100% \n",
      "mkl-service-2.3.0    | 208 KB    | ########## | 100% \n",
      "libsndfile-1.0.29    | 534 KB    | ########## | 100% \n",
      "libvorbis-1.3.7      | 549 KB    | ########## | 100% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tornado-6.0.4        | 650 KB    | ########## | 100% \n",
      "blas-1.0             | 6 KB      | ########## | 100% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "cryptography-2.3.1   | 585 KB    | ########## | 100% \n",
      "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
      "idna-2.10            | 56 KB     | ########## | 100% \n",
      "libvpx-1.7.0         | 2.4 MB    | ########## | 100% \n",
      "libopus-1.3.1        | 570 KB    | ########## | 100% \n",
      "six-1.15.0           | 13 KB     | ########## | 100% \n",
      "gcc_impl_linux-64-7. | 70.6 MB   | #########9 | 100% \n",
      "gcc_impl_linux-64-7. | 70.6 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "Pip subprocess error:\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /azureml-envs/azureml_6b5289b2dc5454b144f8bed30cb22bbb/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-yytzeewr/obspy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-yytzeewr/obspy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-tmd4avje\n",
      "         cwd: /tmp/pip-install-yytzeewr/obspy/\n",
      "    Complete output (5 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-yytzeewr/obspy/setup.py\", line 47, in <module>\n",
      "        from numpy.distutils.core import DistutilsSetupError, setup\n",
      "    ImportError: cannot import name 'DistutilsSetupError'\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "\n",
      "\n",
      "CondaEnvException: Pip failed\n",
      "\n",
      "\u001b[0mRan pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_6b5289b2dc5454b144f8bed30cb22bbb/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.d9djbaoq.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting azureml-defaults~=1.24.0\n",
      "  Downloading azureml_defaults-1.24.0-py3-none-any.whl (3.1 kB)\n",
      "Collecting azureml==0.2.7\n",
      "  Downloading azureml-0.2.7-py2.py3-none-any.whl (23 kB)\n",
      "Collecting azureml-core~=1.24.0\n",
      "  Downloading azureml_core-1.24.0.post2-py3-none-any.whl (2.2 MB)\n",
      "Collecting azureml-sdk~=1.24.0\n",
      "  Downloading azureml_sdk-1.24.0-py3-none-any.whl (4.4 kB)\n",
      "Collecting azureml-automl-core~=1.24.0\n",
      "  Downloading azureml_automl_core-1.24.0-py3-none-any.whl (195 kB)\n",
      "Collecting azureml-automl-runtime~=1.24.0\n",
      "  Downloading azureml_automl_runtime-1.24.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting azure-storage-blob==12.6.0\n",
      "  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\n",
      "Collecting numpy==1.18.5\n",
      "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting sqlalchemy==1.3.21\n",
      "  Downloading SQLAlchemy-1.3.21-cp36-cp36m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "Collecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting tqdm==4.54.0\n",
      "  Downloading tqdm-4.54.0-py2.py3-none-any.whl (69 kB)\n",
      "Collecting obspy==1.2.1\n",
      "  Downloading obspy-1.2.1.zip (24.7 MB)\n",
      "\n",
      "The command '/bin/sh -c ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_6b5289b2dc5454b144f8bed30cb22bbb -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig' returned a non-zero code: 1\n",
      "2021/05/17 00:59:21 Container failed during run: acb_step_0. No retries remaining.\n",
      "failed to run step ID: acb_step_0: exit status 1\n",
      "\n",
      "Run ID: de3c failed after 5m35s. Error: failed during run, err: exit status 1\n",
      "Package creation Failed\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Package creation reached non-successful terminal state.\nState: Failed\nError:\nStatusCode: 400\nMessage: Environment image build failed.\n\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Package creation reached non-successful terminal state.\\nState: Failed\\nError:\\nStatusCode: 400\\nMessage: Environment image build failed.\\n\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-698bd8b9605c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpackage_service\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\conda2\\lib\\site-packages\\azureml\\core\\model.py\u001b[0m in \u001b[0;36mwait_for_creation\u001b[1;34m(self, show_output)\u001b[0m\n\u001b[0;32m   2730\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2732\u001b[1;33m             raise WebserviceException('Package creation reached non-successful terminal state.\\n'\n\u001b[0m\u001b[0;32m   2733\u001b[0m                                       \u001b[1;34m'State: {}\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2734\u001b[0m                                       \u001b[1;34m'Error:\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Package creation reached non-successful terminal state.\nState: Failed\nError:\nStatusCode: 400\nMessage: Environment image build failed.\n\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Package creation reached non-successful terminal state.\\nState: Failed\\nError:\\nStatusCode: 400\\nMessage: Environment image build failed.\\n\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "package_service.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facial-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "print(package_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "related-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsmcdevmlwsfe4aa203.azurecr.io/mc-mlmodule@sha256:82815f39b4278e44ac9dfbea9ac3f145aa0111bf7879206bde0c7cec6190c207\n"
     ]
    }
   ],
   "source": [
    "print(package_service.location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
